---
title: 私有知识库构建方案
---

![LLM Evolutionary Tree](https://pic4.zhimg.com/v2-a285cd76374ece7f56a7b7e450ae3fab_b.jpg)

#### 基础概述与拓展

>基于Transformer架构的模型主要三大分类:
>
>1、autoregressive自回归模型（AR模型,仅解码器架构Decoder-only）：代表作GPT。本质上是一个left-to-right的语言模型。通常用于生成式任务，在长文本生成方面取得了巨大的成功，比如自然语言生成（NLG）领域的任务：摘要、翻译或抽象问答。当扩展到十亿级别参数时，表现出了少样本学习能力。缺点是单向注意力机制，在NLU任务中，无法完全捕捉上下文的依赖关系。
>
>2、autoencoding自编码模型（AE模型,仅编码器架构Encoder-only）：代表作BERT。是通过某个降噪目标（比如MLM）训练的双向文本编码器。编码器会产出适用于NLU任务的上下文表示，但无法直接用于文本生成。
>
>3、encoder-decoder（Seq2seq模型）：代表作T5。采用双向注意力机制，通常用于条件生成任务，比如文本摘要、机器翻译等
>
>**[为什么现在的LLM都是Decoder-only的架构？](https://kexue.fm/archives/9529)**
>
>**[为什么现在的LLM都是Decoder only的架构？](https://www.zhihu.com/question/588325646/answer/2940298964)**
>
>**[生成式预训练模型：UniLM、BART、T5、GPT](https://zhuanlan.zhihu.com/p/406751681)**
>
>**[大语言模型调研汇总](https://zhuanlan.zhihu.com/p/614766286)**
>
>**[从循环神经网络、transformer到GPT2](https://blog.csdn.net/qq_56591814/article/details/119759105)**
>
>**[循环神经网络讲解（RNN/LSTM/GRU）](https://zhuanlan.zhihu.com/p/123211148)**
>
>**[LLMs模型速览上（GPTs、LaMDA、GLM/ChatGLM、PaLM/Flan-PaLM）](https://zhuanlan.zhihu.com/p/637985826)**



#### ChatGPT +Fine-tune

>ChatGPT的唯一方式就是官方说的微调出一个自己的模型，然后上传到ChatGPT，聊天时选用这个模型就好
>
>参考文档:
>
>[如何微调你的chatgpt？fine-tuning实例教学](https://www.youtube.com/watch?v=mzz1ldrRcuc)
>
>[openai fine-tuning guides](https://platform.openai.com/docs/guides/fine-tuning/fine-tuning)

​	

#### Azure  Studio 训练自己的GPT

>[Azure OpenAI 微调模型文档](https://learn.microsoft.com/zh-cn/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio)



#### 开源LLM + 本地部署和微调

##### 开源大模型

###### ChatGLM

>清华和智谱公司搞的开源LLM,ChatGLM-6B 是一个开源的、支持中英双语的对话语言模型，基于 **[General Language Model (GLM)](https://link.zhihu.com/?target=https%3A//github.com/THUDM/GLM)** 架构，具有 62 亿参数。结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4 量化级别下最低只需 6GB 显存）。 ChatGLM-6B 使用了和 ChatGPT 相似的技术，针对中文问答和对话进行了优化。经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62 亿参数的 ChatGLM-6B 已经能生成相当符合人类偏好的回答。
>
>**[ChatGLM3-6B针对RAG能力对比测试](https://luxiangdong.com/2023/10/30/chatglm3/#/ChatGLM3-6B%E6%9D%A5%E4%BA%86)**
>
>**github:**[ChatGLM3-6B](https://github.com/THUDM/ChatGLM3)
>
>**官方体验地址:**[智谱清言](https://www.chatglm.cn/)
>
>**[清华ChatGLM底层原理详解](https://zhuanlan.zhihu.com/p/630134021)**
>
>**[ChatGLM-blog](https://chatglm.cn/blog)**

###### Alpaca

>**Alpaca 7B是由Meta的LLaMA 7B模型通过52K指令微调得到的模型。Alpaca与OpenAI的text-davinci-003（GPT-3.5）表现类似，模型容量惊人的小，易于复现，且复现成本低（<600美元）。**斯坦福发布了模型训练方法和数据，并打算在未来发布模型权重
>
>**[github-stanford-alpaca](https://github.com/tatsu-lab/stanford_alpaca)**
>
>**github [alpaca-lora](https://github.com/tloen/alpaca-lora)**
>
>**github [Chinese-LLAMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca)**
>
>[Alpaca-lora本地化部署实践](https://zhuanlan.zhihu.com/p/629998652)



###### Vicuna

>可达到 ChatGPT/Bard 90% 以上水平
>
>来自加州大学伯克利分校、CMU、斯坦福大学和加州大学圣地亚哥分校的成员，共同推出了一个 Vicuna-13B 开源聊天机器人，由增强的数据集和易于使用、可扩展的基础设施支持。
>
>以 GPT-4 为评判标准的初步评估显示，Vicuna-13B 达到了 OpenAI ChatGPT 和 Google Bard 90% 以上的质量，同时在 90% 以上的情况下超过了 LLaMA 和 Stanford Alpaca 等其他模型的表现。训练 Vicuna-13B 成本约为 300 美元。训练和服务代码，以及在线[演示](https://link.zhihu.com/?target=https%3A//www.oschina.net/action/GoToLink%3Furl%3Dhttps%3A%2F%2Fchat.lmsys.org%2F)都是公开的，可用于非商业用途
>
>**[本地化部署Vicuna模型-教程](https://zhuanlan.zhihu.com/p/633246784)**
>
>**[Hugging Face 地址](https://huggingface.co/lmsys/vicuna-13b-v1.5)**
>
>**[Hugging Face lmsys](https://huggingface.co/lmsys)**
>
>**[FastChat-github](https://github.com/lm-sys/FastChat/tree/main/fastchat)**
>
>**[Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90% ChatGPT Quality](https://link.zhihu.com/?target=https%3A//vicuna.lmsys.org/)**



###### LLaMA2

>Meta加微软，开源，免费开放给商业
>
>Meta宣布和微软深化合作，正式推出新一代开源大型语言模型Llama 2，并将该模型免费开放给商业和研究使用。当时Meta表示LLaMA拥有超高性能，使用130亿参数的LLaMA在规模仅为ChatGPT 的十分之一的情况下，性能表现可以优于GPT-3。使用650 亿参数的 LLaMA 则可以与 DeepMind700 亿参数的 Chinchilla模型和谷歌5400 亿参数的 PaLM模型比肩
>
>[lama2模型申请与本地部署详细](https://www.bilibili.com/video/BV1oc411R7Fy/?vd_source=3a5c8a14693f88eaee1ab81779a7f8cb)



####  外部LLM(ChatGPT/ChatGLM3) + 向量数据库

>当我们有一份文档需要 GPT 处理时，例如客服培训资料或者操作手册，可以先将这份文档的所有内容转化成向量（这个过程称之为 Vector Embedding），然后当用户提出相关问题时，我们将用户的搜索内容转换成向量，然后在数据库中搜索最相似的向量，匹配最相似的几个上下文，最后将上下文返回给 GPT。这样不仅可以大大减少 GPT 的计算量，从而提高响应速度，更重要的是降低成本，并绕过 GPT 的 tokens 限制

截至目前，汇总到目前的向量数据库有以下几种选择：

| 向量数据库 | URL                                    | GitHub Star | Language      | Cloud |
| :--------- | :------------------------------------- | :---------- | :------------ | :---- |
| chroma     | https://github.com/chroma-core/chroma  | 7.4K        | Python        | ❌     |
| milvus     | https://github.com/milvus-io/milvus    | 21.5K       | Go/Python/C++ | ✅     |
| pinecone   | https://www.pinecone.io/               | ❌           | ❌             | ✅     |
| qdrant     | https://github.com/qdrant/qdrant       | 11.8K       | Rust          | ✅     |
| typesense  | https://github.com/typesense/typesense | 12.9K       | C++           | ❌     |
| weaviate   | https://github.com/weaviate/weaviate   | 6.9K        | Go            | ✅     |

>[向量数据库技术鉴赏](https://www.bilibili.com/video/BV11a4y1c7SW/)

#### 内部部署开源LLM + 向量数据库

###### Langchain-Chatchat

>Langchain-Chatchat（原Langchain-ChatGLM）基于 Langchain 与 ChatGLM 等语言模型的本地知识库问答 
>
>**github:** [Langchain-Chatchat](https://github.com/chatchat-space/Langchain-Chatchat)
>
>**[ChatGLM + LangChain 实践培训](https://www.bilibili.com/video/BV13M4y1e7cN/?share_source=copy_web&vd_source=e6c5aafe684f30fbe41925d61ca6d514)**
>
>**[ChatGLM+Langchain构建本地知识库，只需6G显存，支持实时上传文档](https://www.bilibili.com/video/BV1t8411y7fp/)**

###### privateGPT

>**github:** [privateGPT](https://github.com/imartinez/privateGPT)

###### FastGPT(不推荐)

>**github:**[FastGPT](https://github.com/labring/FastGPT)

###### 其他方案

>https://zhuanlan.zhihu.com/p/652685138

#### Other

##### gpt对话分享

> [ShareGPT](https://sharegpt.com/)

##### 大语言模型评估排行榜汇总

>LMSYS:[排行榜地址](https://chat.lmsys.org/?leaderboard)
>
>>排行榜由Chatboot Arena、MT-Bench、MMLU主要基准组成。
>>
>>- Chatbot Arena- 一个大语言模型基准平台，以众包的方式进行匿名、随机的战斗。目前有 47K+ 用户的投票数据，采用Elo 评级方法进行计算结果。该项基准正在考虑增加更多的开源/闭源模型。
>>- MT-Bench- 一个具有挑战性的多轮问题集，使用 GPT-4 对模型响应进行评分。
>>- MMLU-（5-shot）——测试大语言模型的多任务准确性，该测试覆盖了基础数学、美国历史、计算机科学、法律等57项任务
>
>c-Eval: [排行榜地址](https://cevalbenchmark.com/static/leaderboard.html)
>
>>C-Eval是一个针对基础模型的综合中文评估套件。它由 13948 道多项选择题组成，涵盖 52 个不同学科和四个难度级别
>
>SuperCLUElyb: [排行榜地址](https://www.superclueai.com/)
>
>>SuperCLUE-琅琊榜是一个中文大模型匿名对战平台，由中文语言理解测评基准开源社区CLUE的成员发起的，目前已有5.8K投票数据，使用Elo评级系统来计算模型的相对性能



instructor: https://github.com/jxnl/instructor

cohere: https://cohere.com

AppAgent: https://github.com/mnotgod96/AppAgent

##### 如何基于Embedding让大模型解决长文本(如PDF)的输入问题

>大多数打语言模型都无法处理过长的文本,如何让大模型"读懂"一个PDF尼
>
>1.基于pdf创建向量Embedding,并在数据库中存储
>
>2.提问Prompt，将问题向量去数据库检索相似的向量,将检索的相似的向量原文一起给到GPT
>
>```
>准备搜索数据（仅一次）
>	收集：即获取你要用的数据，例如OpenAI的案例是下载几百篇有关2022年奥运会的维基百科文章
>	切块：将文档分成短小的、大多是自包含的部分以进行嵌入
>	嵌入：使用OpenAI API对每个部分数据获得embeddings结果
>	存储：存储embedding是（对于大型数据集，可以使用向量数据库）
>搜索（每次查询一次）
>	给定用户问题，从OpenAI API生成查询的embeddings
>	使用embeddings，按照与查询相关性对文本部分进行排序
>提问（每次查询一次）
>	将问题和最相关的部分插入到发送给GPT的消息中
>	返回GPT的答案
>```
>
>

