ML-AndrewNg-Notes：https://zhuanlan.zhihu.com/p/353905922

#### 1.根据数据训练方式:分为监督学习，无监督学习，强化学习
监督学习：supervised learning 数据是有标签的
无监督学习：unsupervised learning  数据没有标签的
强化学习：Reinforcement Algorithms 对没有学习过的问题做出正确解答的泛化能力，可以理解为监督学习+无监督学习


#### 2. 基本的机器学习算法:

线性回归算法(Linear Regression) 
    回归分析是统计学的数据分析方法,目的在于了解两个或多个变量间是否相关,相关方向和强度
    建模过程就是利用数据点寻找最佳拟合曲线
    分为简单线性回归(只有一个自变量),和多变量回归()

支持向量机(Support Vector Machine,SVM)
    属于分类型算法:仅仅适用二分类问题，应用将多类任务需要减少到几个二元问题
    基本模型是定义在特征空间上的间隔最大的线性分类器


KNN(K-Nearest Negighbors)
    有监督学习中的分类算法
    预测一个值时候，距离最近的k个点的那个种类较多，就归类到那个种类
    距离计算: 二维平面使用欧氏距离计算公式
    k值选择: 拆分为训练数据和验证数据来交叉验证

逻辑回归(Logistic Regression)
    用于需要明确输出场景(预测是否会发生降雨)，本质还是用于分类
    因为通过逻辑回归模型，我们得到的计算结果是0-1之间的连续数字,逻辑回归的核心思想是将线性回归的输出结果通过一个拟合函数转换成概率值，从而实现对样本的分类
    拟合函数: sigmond
    损失函数: 用于衡量预测值和实际值的偏移程度
    如何去求解模型中的参数:逻辑回归模型的数学形式确定后，在统计学中，常常使用极大似然估计法来求解，即找到一组参数，使得在这组参数下，我们的数据的似然度（概率）最大
    即在逻辑回归模型中，我们最大化似然函数和最小化损失函数实际上是等价的

    使用方法:
        1.数据预处理：对数据进行归一化处理，将特征值缩放到一个相同的范围内。
        2.构建逻辑回归模型：定义 Sigmoid 函数和损失函数。
        3.训练模型：使用梯度下降法或其他优化算法优化损失函数，得到模型参数 w 和 b。
        4.模型评估：通过计算准确率、召回率等评价指标，评估模型性能。
        5.应用模型：将训练好的模型用于预测新的样本


决策树(Decision Tree)
    决策树是一树状结构，它的每一个叶节点对应着一个分类，非叶节点对应着在某个属性上的划分，根据样本在该属性上的不同取值将其划分成若干个子集,常用来解决分类问题
    简单决策树算法案例：确定人群中谁喜欢使用信用卡。考虑人群的年龄和婚姻状况，如果年龄在30岁或是已婚，人们更倾向于选择信用卡，反之则更少
    一个决策树包含三种类型的节点：
        决策节点：通常用矩形框来表示
        机会节点：通常用圆圈来表示
        终结节点：通常用三角形来表示
    决策树的关键-如何找合适的"划分属性"
        1.信息增益(可以理解为系统由不稳定态到稳定态所需要丢失的部分，信息熵可以理解为信息由不干净到干净所需要丢失的部分)
        2.增益率
        3.基尼系数
        4.剪枝(对方过拟合的主要手段)

k-平均(K-Means)
    K-means 是我们最常用的基于欧式距离的聚类算法，其认为两个目标的距离越近，相似度越大
    算法过程:
        1.选择初始化的 k 个样本作为初始聚类中心 
        2.针对数据集中每个样本,计算它到 k 个聚类中心的距离并将其分到距离最小的聚类中心所对应的类中；
        3.针对每个类别,重新计算它的聚类中心 （即属于该类的所有样本的质心）；
        4.重复上面 2 3 两步操作，直到达到某个中止条件（迭代次数、最小误差变化等）

随机森林(Random Forest)
随机森林可以看作一个决策树的集合,随机森林中每棵决策树估计一个分类，这个过程称为“投票（vote）”。理想情况下，我们根据每棵决策树的每个投票，选择最多投票的分类
将若干个弱分类器的分类结果进行投票选择，从而组成一个强分类器，这就是随机森林bagging的思想


朴素贝叶斯(Navie Bayes)
基于概率论的贝叶斯定理
贝叶斯分类是一类分类算法的总称，这类算法均以贝叶斯定理为基础，故统称为贝叶斯分类。而朴素朴素贝叶斯分类是贝叶斯分类中最简单，也是常见的一种分类方法


降维算法(Dimensional Reduction)
维是指在某些限定条件下，降低随机变量个数，得到一组“不相关”主变量的过程（较本质的解释）
换言之，降维其更深层次的意义在于有效信息的提取综合及无用信息的摈弃


梯度增强算法(Gradient Boosting)
集成学习的思路:
    Bagging
    Boosting（增强集成学习）:Boosting 类的集成学习，主要有：Ada Boosting 和 Gradient Boosting 两类
    Stacking（堆叠）



#### 3.深度学习(神经网络)
人工神经网络（Artificial Neural Networks，ANN）也简称为神经网络，是一种模仿生物神经网络行为特征，进行分布式并行信息处理的数学模型
    M-P神经元模型
    感知机(Perceptron)模型:感知器本质上就是一个通过加权计算函数进行决策的工具
        1.给权重系数置初值；

        2.对于训练集中一个实例的输入值，计算感知机的输出值；

        3.如若感知机的输出值和实例中默认正确的输出值不同：

            (1)若输出值应该为0但实际为1，减少输入值是1的例子的权重；

            (2)若输出值应该为1但实际为0，增加输入值是1的例子的权重；

        4.对于训练集中下一个例子做同样的事，重复步骤2~3，直到感知机不再出错为止；

        5.通过将计算结果馈送到激活函数（or传递函数），把计算结果变成输出信号。
            引入激活函数的目的是在模型中引入非线性因素。没有激活函数的每层都相当于矩阵相乘。就算你叠加了若干层之后，无非还是个矩阵相乘罢了。
        激活函数:
            在多层神经网络中，上层节点的输出和下层节点的输入之间具有一个函数关系，这个函数称为激活函数（又称激励函数/传递函数）
        激活函数分类:
            饱和激活函数（Saturated）和非饱和函数（One-sided / Non-Saturated）
            饱和激活函数:Sigmoid(自身缺陷,不在使用),TanH,Softmax
            非饱和激活函数: ReLU,
    BP神经网络:
        当结构上使用了多层的感知机递接连成一个前向型的网络时，就是一个多层感知机（MLP，Multilayer Perceptron），是一种前馈人工神经网络模型
        BP神经网络，就是在MLP的基础上，引入非线性的激活函数，加入了BP（Back Propagation，反向传播）算法，采用了梯度下降等优化算法进行参数调优的神经网络
        


#### 4.机器学习模型怎么训练的？
一句话理解机器学习过程: 通过有标签样本来调整(学习)并确定所有权重weights和偏差bias的理想值

训练目标:
   最小损失函数(Loss Function:用来度量模型预测值和真实值的差异)
   损失是一个数值,表示对单个样本而言模型预测的准确程度
   模型必须定义损失函数 loss function。例如，线性回归模型通常将均方误差用作损失函数，而逻辑回归模型则使用对数损失函数

训练过程：
   特征 -> 模型 ->推理(执行预测) -> 计算损失(带上标签) ->计算参数更新(以降低损失最小)
   计算参数更新:
      使用梯度下降方法:因为通过计算整个数据集中w每个可能值的损失函数来找到收敛点这种方法效率太低。所以通过梯度能找到损失更小的方向，并迭代
      梯度下降：好比道士下山，如何在一座山顶上，找到最短的路径下山，并且确定最短路径的方向
      原理上就是凸形问题求最优解，因为只有一个最低点；即只存在一个斜率正好为 0 的位置。这个最小值就是损失函数收敛之处
      梯度下降的目标: 寻找梯度下降最快的那个方向
      学习速率(Learning Rate):梯度下降法算法用梯度乘以一个称为学习速率（有时也称为步长）的标量，以确定下一个点的位置。这是超参数，用来调整AI算法速率

#### 5.BP神经网络的训练过程？
BP（BackPropagation）网络的训练，是反向传播算法的过程，是由数据信息的正向传播和误差Error的反向传播两个过程组成
反向传播算法是神经网络算法的核心，其数学原理是：链式求导法则
正向传播过程：
    输入层通过接收输入数据，传递给中间层（各隐藏层）神经元，每一个神经元进行数据处理变换，然后通过最后一个隐藏层传递到输出层对外输出
反向传播过程:
    正向传播后通过真实值和输出值得到误差Error，当Error大于设定值，既实际输出与期望输出差别过大时，进入误差反向传播阶段：
    Error通过输出层，按照误差梯度下降的方式，如上面提到的随机梯度下降法SGD，反向修正各层参数（如Weights），向隐藏层、输入层逐层反转
    通过不断的正向、反向传播，直到输出的误差减少到预定值，或到达最大训练次数