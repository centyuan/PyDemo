---
title: 私有知识库构建方案
---

### 私有知识库构建方案

#### 1.ChatGPT +Fine-tune

>ChatGPT的唯一方式就是官方说的微调出一个自己的模型，然后上传到ChatGPT，聊天时选用这个模型就好
>
>参考文档:
>
>[如何微调你的chatgpt？fine-tuning实例教学](https://www.youtube.com/watch?v=mzz1ldrRcuc)
>
>[openai fine-tuning guides](https://platform.openai.com/docs/guides/fine-tuning/fine-tuning)	

#### 2.Azure  Studio 训练自己的GPT

>[Azure OpenAI 微调模型文档](https://learn.microsoft.com/zh-cn/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio)



#### 3.开源LLM + 本地部署和微调

##### 开源大模型

###### ChatGLM

>清华和智谱公司搞的开源LLM,ChatGLM-6B 是一个开源的、支持中英双语的对话语言模型，基于 **[General Language Model (GLM)](https://link.zhihu.com/?target=https%3A//github.com/THUDM/GLM)** 架构，具有 62 亿参数。结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4 量化级别下最低只需 6GB 显存）。 ChatGLM-6B 使用了和 ChatGPT 相似的技术，针对中文问答和对话进行了优化。经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62 亿参数的 ChatGLM-6B 已经能生成相当符合人类偏好的回答。
>
>**[ChatGLM3-6B针对RAG能力对比测试](https://luxiangdong.com/2023/10/30/chatglm3/#/ChatGLM3-6B%E6%9D%A5%E4%BA%86)**
>
>**github:**[ChatGLM3-6B](https://github.com/THUDM/ChatGLM3)
>
>**官方体验地址:**[智谱清言](https://www.chatglm.cn/)
>
>**[清华ChatGLM底层原理详解](https://zhuanlan.zhihu.com/p/630134021)**
>
>**[ChatGLM-blog](https://chatglm.cn/blog)**

###### Alpaca

>**Alpaca 7B是由Meta的LLaMA 7B模型通过52K指令微调得到的模型。Alpaca与OpenAI的text-davinci-003（GPT-3.5）表现类似，模型容量惊人的小，易于复现，且复现成本低（<600美元）。**斯坦福发布了模型训练方法和数据，并打算在未来发布模型权重
>
>**[github-stanford-alpaca](https://github.com/tatsu-lab/stanford_alpaca)**
>
>**github [alpaca-lora](https://github.com/tloen/alpaca-lora)**
>
>**github [Chinese-LLAMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca)**
>
>[Alpaca-lora本地化部署实践](https://zhuanlan.zhihu.com/p/629998652)



###### Vicuna

>可达到 ChatGPT/Bard 90% 以上水平
>
>来自加州大学伯克利分校、CMU、斯坦福大学和加州大学圣地亚哥分校的成员，共同推出了一个 Vicuna-13B 开源聊天机器人，由增强的数据集和易于使用、可扩展的基础设施支持。
>
>以 GPT-4 为评判标准的初步评估显示，Vicuna-13B 达到了 OpenAI ChatGPT 和 Google Bard 90% 以上的质量，同时在 90% 以上的情况下超过了 LLaMA 和 Stanford Alpaca 等其他模型的表现。训练 Vicuna-13B 成本约为 300 美元。训练和服务代码，以及在线[演示](https://link.zhihu.com/?target=https%3A//www.oschina.net/action/GoToLink%3Furl%3Dhttps%3A%2F%2Fchat.lmsys.org%2F)都是公开的，可用于非商业用途
>
>**[本地化部署Vicuna模型-教程](https://zhuanlan.zhihu.com/p/633246784)**
>
>**[Hugging Face 地址](https://huggingface.co/lmsys/vicuna-13b-v1.5)**
>
>**[Hugging Face lmsys](https://huggingface.co/lmsys)**
>
>**[FastChat-github](https://github.com/lm-sys/FastChat/tree/main/fastchat)**
>
>**[Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90% ChatGPT Quality](https://link.zhihu.com/?target=https%3A//vicuna.lmsys.org/)**



###### LLaMA2

>Meta加微软，开源，免费开放给商业
>
>Meta宣布和微软深化合作，正式推出新一代开源大型语言模型Llama 2，并将该模型免费开放给商业和研究使用。当时Meta表示LLaMA拥有超高性能，使用130亿参数的LLaMA在规模仅为ChatGPT 的十分之一的情况下，性能表现可以优于GPT-3。使用650 亿参数的 LLaMA 则可以与 DeepMind700 亿参数的 Chinchilla模型和谷歌5400 亿参数的 PaLM模型比肩
>
>[lama2模型申请与本地部署详细](https://www.bilibili.com/video/BV1oc411R7Fy/?vd_source=3a5c8a14693f88eaee1ab81779a7f8cb)



####  4.外部LLM(ChatGPT/ChatGLM3) + 向量数据库

>当我们有一份文档需要 GPT 处理时，例如客服培训资料或者操作手册，可以先将这份文档的所有内容转化成向量（这个过程称之为 Vector Embedding），然后当用户提出相关问题时，我们将用户的搜索内容转换成向量，然后在数据库中搜索最相似的向量，匹配最相似的几个上下文，最后将上下文返回给 GPT。这样不仅可以大大减少 GPT 的计算量，从而提高响应速度，更重要的是降低成本，并绕过 GPT 的 tokens 限制

截至目前，汇总到目前的向量数据库有以下几种选择：

| 向量数据库 | URL                                    | GitHub Star | Language      | Cloud |
| :--------- | :------------------------------------- | :---------- | :------------ | :---- |
| chroma     | https://github.com/chroma-core/chroma  | 7.4K        | Python        | ❌     |
| milvus     | https://github.com/milvus-io/milvus    | 21.5K       | Go/Python/C++ | ✅     |
| pinecone   | https://www.pinecone.io/               | ❌           | ❌             | ✅     |
| qdrant     | https://github.com/qdrant/qdrant       | 11.8K       | Rust          | ✅     |
| typesense  | https://github.com/typesense/typesense | 12.9K       | C++           | ❌     |
| weaviate   | https://github.com/weaviate/weaviate   | 6.9K        | Go            | ✅     |

>[向量数据库技术鉴赏](https://www.bilibili.com/video/BV11a4y1c7SW/)

#### 5.内部部署开源LLM + 向量数据库

###### Langchain-Chatchat

>Langchain-Chatchat（原Langchain-ChatGLM）基于 Langchain 与 ChatGLM 等语言模型的本地知识库问答 
>
>**github:** [Langchain-Chatchat](https://github.com/chatchat-space/Langchain-Chatchat)
>
>**[ChatGLM + LangChain 实践培训](https://www.bilibili.com/video/BV13M4y1e7cN/?share_source=copy_web&vd_source=e6c5aafe684f30fbe41925d61ca6d514)**
>
>**[ChatGLM+Langchain构建本地知识库，只需6G显存，支持实时上传文档](https://www.bilibili.com/video/BV1t8411y7fp/)**

###### privateGPT

>**github:** [privateGPT](https://github.com/imartinez/privateGPT)

###### FastGPT(不推荐)

>**github:**[FastGPT](https://github.com/labring/FastGPT)

###### 其他方案

>https://zhuanlan.zhihu.com/p/652685138

#### Other

##### gpt对话分享

> [ShareGPT](https://sharegpt.com/)


